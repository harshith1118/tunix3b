{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V6E1"},"accelerator":"TPU","kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":7045423,"sourceType":"datasetVersion","datasetId":4054119,"isSourceIdPinned":false}],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dharshiiiii/tunix-demo-gemma3-1b?scriptVersionId=289745548\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# GRPO Demo\n\nThis tutorial demonstrates training the [Gemma](https://deepmind.google/models/gemma/)\n3 1B-IT model on the [GSM8K math reasoning benchmark](https://huggingface.co/datasets/openai/gsm8k)\nusing [Group Relative Policy Optimization (GRPO)](https://arxiv.org/pdf/2402.03300).\nGRPO can enhance your model's problem-solving skills on mathematical word problems,\ncoding problems, etc.\n\nGRPO is an RL algorithm designed to enhance the reasoning abilities of LLMs. It\nis a variant of [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347)\nthat reduces memory usage by eliminating the need for a separate value function\nmodel. GRPO works by generating multiple responses for a given prompt,\nevaluating these responses using a reward model, and then calculating a relative\nadvantage based on the group's performance to update the policy.\n\nIn this tutorial we use a `v5e-8` TPU for Gemma3-1b-it. Let's get started!\n\nNote that the setup below is for the Gemma3-1B-IT model only. If you want to use\nanother model (say, Qwen2.5), you may need to change the setup (for example,\ntokenizer, chat template, reward function, etc.).","metadata":{"id":"abdhOBYHqYz6"}},{"cell_type":"code","source":"import os\nos.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:30:25.988067Z","iopub.execute_input":"2025-12-28T02:30:25.988341Z","iopub.status.idle":"2025-12-28T02:30:25.9999Z","shell.execute_reply.started":"2025-12-28T02:30:25.988318Z","shell.execute_reply":"2025-12-28T02:30:25.999103Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Install necessary libraries","metadata":{"id":"afofSj37qYz6"}},{"cell_type":"code","source":"!pip install -q kagglehub -q\n\n!pip install -q ipywidgets -q\n\n!pip install -q tensorflow -q\n!pip install -q tensorflow_datasets -q\n!pip install -q tensorboardX -q\n!pip install -q transformers -q\n!pip install -q grain -q\n!pip install \"google-tunix[prod]==0.1.3\" -q\n\n# !pip install -q git+https://github.com/google/tunix\n# !pip install -q git+https://github.com/google/qwix\n\n!pip uninstall -q -y flax -q\n# !pip install -U flax\n!pip install flax==0.12.0 -q\n\n!pip install -q datasets wandb==0.22.0 -q","metadata":{"id":"Z03GnyApTn1j","outputId":"ccd485a1-84b1-4d9f-bf45-4bf4a4c8ee7b","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:30:34.907708Z","iopub.execute_input":"2025-12-28T02:30:34.907966Z","iopub.status.idle":"2025-12-28T02:31:08.392012Z","shell.execute_reply.started":"2025-12-28T02:30:34.907945Z","shell.execute_reply":"2025-12-28T02:31:08.390866Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# import wandb, os\n# from kaggle_secrets import UserSecretsClient\n# os.environ['WANDB_API_KEY'] = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"id":"953CnpN5xQc_","outputId":"28f62af4-2d67-4034-dc33-d0f69ddf88cc","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:32:41.324047Z","iopub.execute_input":"2025-12-28T02:32:41.324351Z","iopub.status.idle":"2025-12-28T02:32:41.327753Z","shell.execute_reply.started":"2025-12-28T02:32:41.324332Z","shell.execute_reply":"2025-12-28T02:32:41.32692Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"LnF9ZACiTn1k"}},{"cell_type":"code","source":"import functools\nimport gc\nimport os\nfrom pprint import pprint\nimport re\n\nimport csv\nimport shutil\n\nfrom flax import nnx\nimport grain\nimport humanize\nimport jax\nimport jax.numpy as jnp\nimport kagglehub\nimport optax\nfrom orbax import checkpoint as ocp\nfrom pathlib import Path\nimport qwix\nimport tensorflow_datasets as tfds\nfrom tqdm.auto import tqdm\nfrom tunix.generate import sampler as sampler_lib\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\n# from tunix.models.gemma3 import model as gemma_lib\n# from tunix.models.gemma3 import params as params_lib\nfrom tunix.models.gemma3 import params\nfrom tunix.models.gemma3 import model\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.sft import metrics_logger\nfrom datasets import load_dataset","metadata":{"id":"McTNo_r8Tn1k","outputId":"67f038b8-509b-46ed-b027-9b72ed7b628c","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:32:50.309569Z","iopub.execute_input":"2025-12-28T02:32:50.309816Z","iopub.status.idle":"2025-12-28T02:33:13.694276Z","shell.execute_reply.started":"2025-12-28T02:32:50.309799Z","shell.execute_reply":"2025-12-28T02:33:13.693164Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"JAX devices:\", jax.devices())\nprint(\"Tunix OK:\", sampler_lib is not None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:34:11.139329Z","iopub.execute_input":"2025-12-28T02:34:11.139827Z","iopub.status.idle":"2025-12-28T02:34:20.709756Z","shell.execute_reply.started":"2025-12-28T02:34:11.139808Z","shell.execute_reply":"2025-12-28T02:34:20.708544Z"}},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1766889252.351047      12 common_lib.cc:648] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:238\n","output_type":"stream"},{"name":"stdout","text":"JAX devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=4, process_index=0, coords=(0,2,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(1,2,0), core_on_chip=0), TpuDevice(id=6, process_index=0, coords=(0,3,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,3,0), core_on_chip=0)]\nTunix OK: True\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Hyperparameters\n\nLet's define the configuration we are going to use. Note that this is by no\nmeans a \"perfect\" set of hyperparameters. To get good results, you might have\nto train the model for longer.","metadata":{"id":"Eu_NI9nHTn1k"}},{"cell_type":"code","source":"# ====== Data ======\nTRAIN_DATA_DIR = \"./data/train\"\nTEST_DATA_DIR = \"./data/test\"\nTRAIN_FRACTION = 1.0\n\n# ====== LoRA ======\nRANK = 64\nALPHA = 64.0\n\n# ====== Sharding ======\nMESH = [(1, 4), (\"fsdp\", \"tp\")]\n\n# ====== GRPO ======\n# === Generation during GRPO training ===\nMAX_PROMPT_LENGTH = 256\nTOTAL_GENERATION_STEPS = 512\n# Important to keep a high-ish temperature for varied, diverse responses during\n# training.\nTEMPERATURE = 0.9\nTOP_P = 1.0\nTOP_K = 50\n# The number of times the policy generates multiple responses for a given prompt\n# within a single training step. This corresponds to `G` in Algorithm 1 in the\n# paper. The \"group\" in GRPO comes from here.\nNUM_GENERATIONS = 4\n\n# === other GRPO configs ===\n# The number of iterations per batch (ùúá in GRPO algo 1).\nNUM_ITERATIONS = 1\n# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n# Important to keep a high enough value for this, otherwise, the KL divergence\n# can increase unchecked.\nBETA = 0.08\n# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n# stable updates.\nEPSILON = 0.2\n\n# ====== Training ======\nTRAIN_MICRO_BATCH_SIZE = 4\n# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\nNUM_BATCHES = 3738\n# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n# increased to a max. of 330 (if batch size is 4).\nNUM_TEST_BATCHES = 100\n\nEVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\nNUM_EPOCHS = 1  # can potentially train for more epochs\n\n# Number of training steps.\nMAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n\n# === AdamW, warmup, cosine scheduler ===\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\n# == Cosine decay with warmup scheduler ==\n# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n# steps, and then gradually decrease the learning rate to 0 using cosine\n# scheduler.\nWARMUP_STEPS = 0.1 * MAX_STEPS\n# == Grad clipping ==\n# Grad clipping to prevent large gradients. Found this\n# important to keep KL divergence in check.\nMAX_GRAD_NORM = 0.1\n\n# Checkpoint saving\nINTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\nCKPT_DIR = \"/tmp/content/ckpts/\"\nSAVE_INTERVAL_STEPS = 500\nMAX_TO_KEEP = 4\n\n# ====== Inference ======\nGENERATION_CONFIGS = {\n    # greedy search\n    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n    # some randomness\n    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n    # liberal\n    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n}","metadata":{"id":"ZPPKme47Tn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:34:31.534566Z","iopub.execute_input":"2025-12-28T02:34:31.534838Z","iopub.status.idle":"2025-12-28T02:34:31.541227Z","shell.execute_reply.started":"2025-12-28T02:34:31.534819Z","shell.execute_reply":"2025-12-28T02:34:31.540185Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Utility functions","metadata":{"id":"ngjtE-63Tn1k"}},{"cell_type":"code","source":"def show_hbm_usage():\n  \"\"\"Displays memory usage per device.\"\"\"\n  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n\n  for d in jax.local_devices():\n    stats = d.memory_stats()\n    used = stats[\"bytes_in_use\"]\n    limit = stats[\"bytes_limit\"]\n    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")","metadata":{"id":"wjMFOr7aTn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:34:45.25983Z","iopub.execute_input":"2025-12-28T02:34:45.260091Z","iopub.status.idle":"2025-12-28T02:34:45.264358Z","shell.execute_reply.started":"2025-12-28T02:34:45.260072Z","shell.execute_reply":"2025-12-28T02:34:45.263293Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Data preprocessing\n\nFirst, let's define some special tokens. We instruct the model to first reason\nbetween the `<reasoning>` and `</reasoning>` tokens. After\nreasoning, we expect it to provide the answer between the `<answer>` and\n`</answer>` tokens.","metadata":{"id":"6BtpYMlaTn1k"}},{"cell_type":"code","source":"reasoning_start = \"<reasoning>\"\nreasoning_end = \"</reasoning>\"\nsolution_start = \"<answer>\"\nsolution_end = \"</answer>\"\n\n\nSYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\nprovide your reasoning. Place it between {reasoning_start} and \\\n{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\nvalue) between {solution_start} and {solution_end}.\"\"\"\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\"\"\"","metadata":{"id":"h6RGv1kSTn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:34:56.154461Z","iopub.execute_input":"2025-12-28T02:34:56.154791Z","iopub.status.idle":"2025-12-28T02:34:56.158461Z","shell.execute_reply.started":"2025-12-28T02:34:56.154771Z","shell.execute_reply":"2025-12-28T02:34:56.157534Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(SYSTEM_PROMPT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:35:09.149482Z","iopub.execute_input":"2025-12-28T02:35:09.149914Z","iopub.status.idle":"2025-12-28T02:35:09.153929Z","shell.execute_reply.started":"2025-12-28T02:35:09.149886Z","shell.execute_reply":"2025-12-28T02:35:09.152995Z"}},"outputs":[{"name":"stdout","text":"You are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final answer (i.e., just one numerical value) between <answer> and </answer>.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"We use OpenAI's [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k), which comprises grade school math word problems.","metadata":{"id":"WASP9N5JTn1k"}},{"cell_type":"code","source":"def extract_hash_answer(text: str) -> str | None:\n  if \"####\" not in text:\n    return None\n  return text.split(\"####\")[1].strip()\n\n\ndef _load_from_tfds(data_dir: str, split: str):\n  import tensorflow_datasets.text.gsm8k\n  return tfds.data_source(\n      \"gsm8k\",\n      split=split,\n      data_dir=data_dir,\n      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n      download=True,\n  )\n\n\ndef download_kaggle_dataset(target_dir=\"./data/gsm8k\"):\n  os.makedirs(target_dir, exist_ok=True)\n  src = kagglehub.dataset_download(\"thedevastator/grade-school-math-8k-q-a\")\n  src = Path(src)\n  dst = Path(target_dir)\n\n  for csv_file in src.glob(\"*.csv\"):  # match all CSV files\n    shutil.copy2(csv_file, dst / csv_file.name)\n    print(f\"Copied {csv_file.name} ‚Üí {dst/csv_file.name}\")\n  return target_dir\n\n\ndef get_dataset(data_dir, split=\"train\", source=\"tfds\") -> grain.MapDataset:\n  # Download data\n  if not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n  if source == \"tfds\":\n    import tensorflow_datasets.text.gsm8k\n    data = tfds.data_source(\n        \"gsm8k\",\n        split=split,\n        data_dir=data_dir,\n        builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n        download=True,\n    )\n\n  elif source == \"kaggle\":\n    kaggle_dir = download_kaggle_dataset(data_dir)\n    file_name = \"main_\" + split + \".csv\"\n    csv_path = os.path.join(kaggle_dir, file_name)  # adjust filename if needed\n\n    data = []\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n      reader = csv.DictReader(csvfile)\n      for row in reader:\n        data.append({\n            \"question\": row[\"question\"],\n            \"answer\": row[\"answer\"],\n        })\n\n  elif source == \"huggingface\":    \n    os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n    data = load_dataset(\"gsm8k\", \"main\", split=split)\n      \n  else:\n    raise ValueError(f\"Unknown source: {source}\")\n\n  def _as_text(v):\n    return v if isinstance(v, str) else v.decode(\"utf-8\")\n\n  dataset = (\n      grain.MapDataset.source(data)\n      .shuffle(seed=42)\n      .map(\n          lambda x: {\n              # passed to model forward pass\n              \"prompts\": TEMPLATE.format(\n                  system_prompt=SYSTEM_PROMPT,\n                  question=_as_text(x[\"question\"]),\n              ),\n              # passed to reward functions\n              \"question\": _as_text(x[\"question\"]),\n              # passed to reward functions\n              \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n          }\n      )\n  )\n  return dataset","metadata":{"id":"gTGjcSMNTn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:35:20.319604Z","iopub.execute_input":"2025-12-28T02:35:20.319837Z","iopub.status.idle":"2025-12-28T02:35:20.327593Z","shell.execute_reply.started":"2025-12-28T02:35:20.319822Z","shell.execute_reply":"2025-12-28T02:35:20.326541Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!ls ../input/grade-school-math-8k-q-a/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:35:32.103965Z","iopub.execute_input":"2025-12-28T02:35:32.104229Z","iopub.status.idle":"2025-12-28T02:35:32.664051Z","shell.execute_reply.started":"2025-12-28T02:35:32.10421Z","shell.execute_reply":"2025-12-28T02:35:32.662803Z"}},"outputs":[{"name":"stdout","text":"main_test.csv  main_train.csv  socratic_test.csv  socratic_train.csv\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('../input/grade-school-math-8k-q-a/socratic_test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:35:37.291069Z","iopub.execute_input":"2025-12-28T02:35:37.291353Z","iopub.status.idle":"2025-12-28T02:35:37.32976Z","shell.execute_reply.started":"2025-12-28T02:35:37.291337Z","shell.execute_reply":"2025-12-28T02:35:37.328723Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df['question'].values[3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:35:47.681217Z","iopub.execute_input":"2025-12-28T02:35:47.681506Z","iopub.status.idle":"2025-12-28T02:35:47.688215Z","shell.execute_reply.started":"2025-12-28T02:35:47.681486Z","shell.execute_reply":"2025-12-28T02:35:47.687328Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'James decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week?'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"df['answer'][3].split('####')[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:35:51.810722Z","iopub.execute_input":"2025-12-28T02:35:51.810991Z","iopub.status.idle":"2025-12-28T02:35:51.815352Z","shell.execute_reply.started":"2025-12-28T02:35:51.810973Z","shell.execute_reply":"2025-12-28T02:35:51.814543Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"' 540'"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"We split the dataset set into train and test sets as usual.","metadata":{"id":"uDwobMu_okwv"}},{"cell_type":"code","source":"# source = input(\"Choose data source [tfds/kaggle]: \").strip().lower()\nsource = \"huggingface\"\n\nif source not in (\"tfds\", \"kaggle\", \"huggingface\"):\n  print(\"Invalid choice. Defaulting to 'tfds'.\")\n  source = \"\"\n\nprint(f\"Using data source: {source}\")\n\ndataset = get_dataset(TRAIN_DATA_DIR, \"train\", source).batch(TRAIN_MICRO_BATCH_SIZE)[\n    :NUM_BATCHES\n]\n\nif TRAIN_FRACTION == 1.0:\n  train_dataset = dataset.repeat(NUM_EPOCHS)\n  val_dataset = None\nelse:\n  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n\n  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n\ntest_dataset = get_dataset(TEST_DATA_DIR, \"test\", source).batch(TRAIN_MICRO_BATCH_SIZE)[\n    :NUM_TEST_BATCHES\n]\n\ndataset_lengths = (\n    len(train_dataset),\n    len(val_dataset) if val_dataset is not None else 0,\n    len(test_dataset),\n)\nprint(f\"dataset contains {dataset_lengths} of batches\")","metadata":{"id":"KXhOL6GyTn1k","outputId":"5e15f893-33eb-42e9-f4bd-20be01f2314a","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:36:00.910768Z","iopub.execute_input":"2025-12-28T02:36:00.911011Z","iopub.status.idle":"2025-12-28T02:36:06.863676Z","shell.execute_reply.started":"2025-12-28T02:36:00.910995Z","shell.execute_reply":"2025-12-28T02:36:06.862542Z"}},"outputs":[{"name":"stdout","text":"Using data source: huggingface\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e420ebf8959d4ed68ea77cae67fbb761"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae36a0da01524a90b9feb8400dd9edfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06e865e2c41f4e8192cd438713651a17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae81f372bf8544378029b07b21f7eb80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a0a7aeda9114f99a4b254480fdf6d5b"}},"metadata":{}},{"name":"stdout","text":"dataset contains (1869, 0, 100) of batches\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"for ele in train_dataset[:1]:\n  pprint(ele)","metadata":{"id":"5TF-wNQ2Tn1k","outputId":"367cd3ef-9b1c-469d-b50c-71887b040e87","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:36:15.02101Z","iopub.execute_input":"2025-12-28T02:36:15.021781Z","iopub.status.idle":"2025-12-28T02:36:15.028768Z","shell.execute_reply.started":"2025-12-28T02:36:15.021758Z","shell.execute_reply":"2025-12-28T02:36:15.027988Z"}},"outputs":[{"name":"stdout","text":"{'answer': array(['3', '34', '300', '35'], dtype='<U3'),\n 'prompts': array(['<start_of_turn>user\\nYou are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final answer (i.e., just one numerical value) between <answer> and </answer>.\\n\\nMaria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?<end_of_turn>\\n<start_of_turn>model',\n       '<start_of_turn>user\\nYou are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final answer (i.e., just one numerical value) between <answer> and </answer>.\\n\\nA wildlife team is monitoring the number of birds in a park. There are 3 blackbirds in each of the park‚Äôs 7 trees. There are also 13 magpies roaming around the park. How many birds are in the park in total?<end_of_turn>\\n<start_of_turn>model',\n       '<start_of_turn>user\\nYou are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final answer (i.e., just one numerical value) between <answer> and </answer>.\\n\\nLast year, the school library purchased 50 new books. This year, it purchased 3 times as many books. If the library had 100 books before it purchased new books last year, how many books are in the library now?<end_of_turn>\\n<start_of_turn>model',\n       '<start_of_turn>user\\nYou are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final answer (i.e., just one numerical value) between <answer> and </answer>.\\n\\nJame gets 20 singing lessons.  He gets the first lesson free and after the first 10 paid lessons he only needs to pay for every other lesson.  Each lesson is $5.  His uncle pays for half.  How much does James pay?<end_of_turn>\\n<start_of_turn>model'],\n      dtype='<U488'),\n 'question': array(['Maria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?',\n       'A wildlife team is monitoring the number of birds in a park. There are 3 blackbirds in each of the park‚Äôs 7 trees. There are also 13 magpies roaming around the park. How many birds are in the park in total?',\n       'Last year, the school library purchased 50 new books. This year, it purchased 3 times as many books. If the library had 100 books before it purchased new books last year, how many books are in the library now?',\n       'Jame gets 20 singing lessons.  He gets the first lesson free and after the first 10 paid lessons he only needs to pay for every other lesson.  Each lesson is $5.  His uncle pays for half.  How much does James pay?'],\n      dtype='<U213')}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"ele['question']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:36:24.320838Z","iopub.execute_input":"2025-12-28T02:36:24.321157Z","iopub.status.idle":"2025-12-28T02:36:24.325956Z","shell.execute_reply.started":"2025-12-28T02:36:24.321138Z","shell.execute_reply":"2025-12-28T02:36:24.325044Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array(['Maria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?',\n       'A wildlife team is monitoring the number of birds in a park. There are 3 blackbirds in each of the park‚Äôs 7 trees. There are also 13 magpies roaming around the park. How many birds are in the park in total?',\n       'Last year, the school library purchased 50 new books. This year, it purchased 3 times as many books. If the library had 100 books before it purchased new books last year, how many books are in the library now?',\n       'Jame gets 20 singing lessons.  He gets the first lesson free and after the first 10 paid lessons he only needs to pay for every other lesson.  Each lesson is $5.  His uncle pays for half.  How much does James pay?'],\n      dtype='<U213')"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(ele['prompts'][2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:36:36.546226Z","iopub.execute_input":"2025-12-28T02:36:36.546674Z","iopub.status.idle":"2025-12-28T02:36:36.550483Z","shell.execute_reply.started":"2025-12-28T02:36:36.546652Z","shell.execute_reply":"2025-12-28T02:36:36.549539Z"}},"outputs":[{"name":"stdout","text":"<start_of_turn>user\nYou are given a problem. Think about the problem and provide your reasoning. Place it between <reasoning> and </reasoning>. Then, provide the final answer (i.e., just one numerical value) between <answer> and </answer>.\n\nLast year, the school library purchased 50 new books. This year, it purchased 3 times as many books. If the library had 100 books before it purchased new books last year, how many books are in the library now?<end_of_turn>\n<start_of_turn>model\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"ele['answer']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:36:48.667634Z","iopub.execute_input":"2025-12-28T02:36:48.667961Z","iopub.status.idle":"2025-12-28T02:36:48.672217Z","shell.execute_reply.started":"2025-12-28T02:36:48.66794Z","shell.execute_reply":"2025-12-28T02:36:48.671507Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array(['3', '34', '300', '35'], dtype='<U3')"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"Let's see how one batch of the training dataset looks like!\n","metadata":{"id":"k7n8L0VzTn1k"}},{"cell_type":"markdown","source":"## Load the policy model and the reference model\n\nThe policy model is the model which is actually trained and whose weights are\nupdated. The reference model is the model with which we compute KL divergence.\nThis is to ensure that the policy updates are not huge and that it does not\ndeviate too much from the reference model.\n\nTypically, the reference model is the base model, and the policy model is the\nsame base model, but with LoRA parameters. Only the LoRA parameters are updated.\n\nNote: We perform full precision (fp32) training. You can, however, leverage\nQwix for QAT.\n\nTo load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\nto have agreed to the Gemma license\n[here](https://www.kaggle.com/models/google/gemma/flax/).","metadata":{"id":"BZxBR7Y_Tn1k"}},{"cell_type":"code","source":"# Log in\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n  kagglehub.login()","metadata":{"id":"3GfLHHVYHHKO","outputId":"3290c9e8-2362-44a1-93bf-9a5caa7d201f","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:38:12.476631Z","iopub.execute_input":"2025-12-28T02:38:12.476949Z","iopub.status.idle":"2025-12-28T02:38:12.488501Z","shell.execute_reply.started":"2025-12-28T02:38:12.476929Z","shell.execute_reply":"2025-12-28T02:38:12.487546Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd7b3d1f9f0d410aab35dc12edb24353"}},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"This code snippet serves as a workaround to re-save the pre-trained model checkpoint from Kaggle into a local format that is compatible with the [Flax NNX](https://flax.readthedocs.io/en/stable/why.html) library. Because the original checkpoint has parameter names and tensor structures that don't match the target NNX model architecture, it cannot be loaded directly.\n\nWe first load the original weights into a temporary model instance, then extract and re-save the model's state into a new, properly formatted local checkpoint, which can then be successfully loaded by the final sharded NNX model.","metadata":{"id":"nAghcsT_Pmv_"}},{"cell_type":"code","source":"!rm /tmp/content/intermediate_ckpt/* -rf\n\n!rm /tmp/content/ckpts/* -rf\n\nmodel_family = \"gemma3\"\nif model_family == \"gemma3\":\n  MODEL_CP_PATH = params.GEMMA3_1B_IT\n  config = model.ModelConfig.gemma3_1b()\n  gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n  tokenizer = params.create_tokenizer()\n\n  checkpointer = ocp.StandardCheckpointer()\n  _, state = nnx.split(gemma)\n  checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n  checkpointer.wait_until_finished()\n  # Delete the intermediate model to save memory.\n  del params\n  del gemma\n  del state\n  gc.collect()","metadata":{"id":"cIFAxgVOTn1k","outputId":"0235a0e1-9f7d-428c-c16e-0bc9e49c2f2f","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:38:57.120754Z","iopub.execute_input":"2025-12-28T02:38:57.120978Z","iopub.status.idle":"2025-12-28T02:39:43.160604Z","shell.execute_reply.started":"2025-12-28T02:38:57.120962Z","shell.execute_reply":"2025-12-28T02:39:43.159245Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nWARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\nE1228 02:39:23.893217    1137 google_auth_provider.cc:188] Could not find the credentials file in the standard gcloud location [/root/.config/gcloud/application_default_credentials.json]. You may specify a credentials file using $GOOGLE_APPLICATION_CREDENTIALS, or to use Google application default credentials, run: gcloud auth application-default login\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Model Loading and LoRA Application\n\nThese two functions work together to load a base model from a checkpoint and apply a LoRA (Low-Rank Adaptation) layer to it.\n\n* `get_ref_model`: Loads the complete Gemma model from a specified checkpoint path. It uses **JAX sharding** to distribute the model parameters across multiple devices.\n* `get_lora_model`: Takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it's ready for distributed training.","metadata":{"id":"hpgXONuORkkq"}},{"cell_type":"code","source":"from tunix.models.gemma3 import params\n\ndef get_gemma_ref_model(ckpt_path):\n  mesh = jax.make_mesh(*MESH)\n  model_config = model.ModelConfig.gemma3_1b()\n  abs_gemma: nnx.Module = nnx.eval_shape(\n      lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n  )\n\n  abs_state = nnx.state(abs_gemma)\n  abs_state = jax.tree.map(\n      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n      abs_state,\n      nnx.get_named_sharding(abs_state, mesh),\n  )\n  checkpointer = ocp.StandardCheckpointer()\n  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n\n  graph_def, _ = nnx.split(abs_gemma)\n  gemma = nnx.merge(graph_def, restored_params)\n  return gemma, mesh, model_config\n\n\ndef get_lora_model(base_model, mesh):\n  lora_provider = qwix.LoraProvider(\n      module_path=(\n          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n          \".*attn_vec_einsum\"\n      ),\n      rank=RANK,\n      alpha=ALPHA,\n  )\n\n  model_input = base_model.get_model_input()\n  lora_model = qwix.apply_lora_to_model(\n      base_model, lora_provider, **model_input\n  )\n\n  with mesh:\n    state = nnx.state(lora_model)\n    pspecs = nnx.get_partition_spec(state)\n    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n    nnx.update(lora_model, sharded_state)\n\n  return lora_model","metadata":{"id":"m2KD-nmbTn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:39:51.376072Z","iopub.execute_input":"2025-12-28T02:39:51.376364Z","iopub.status.idle":"2025-12-28T02:39:51.38235Z","shell.execute_reply.started":"2025-12-28T02:39:51.376346Z","shell.execute_reply":"2025-12-28T02:39:51.381268Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"Now we load reference and policy Gemma models using the Flax NNX library and display their structures.","metadata":{"id":"mgBALRieR6aY"}},{"cell_type":"code","source":"# Reference model\nif model_family == \"gemma3\":\n  ref_model, mesh, model_config = get_gemma_ref_model(\n      ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n  )","metadata":{"id":"kSdZ7aGhTn1k","outputId":"a536819f-dd5f-4e29-8ebe-09234960c114","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:40:02.966465Z","iopub.execute_input":"2025-12-28T02:40:02.966777Z","iopub.status.idle":"2025-12-28T02:40:08.011773Z","shell.execute_reply.started":"2025-12-28T02:40:02.966757Z","shell.execute_reply":"2025-12-28T02:40:08.010708Z"}},"outputs":[{"name":"stderr","text":"WARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Policy model\nlora_policy = get_lora_model(ref_model, mesh=mesh)\n# nnx.display(lora_policy)","metadata":{"id":"4i3CfJ1gTn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:40:21.045422Z","iopub.execute_input":"2025-12-28T02:40:21.045756Z","iopub.status.idle":"2025-12-28T02:40:28.7983Z","shell.execute_reply.started":"2025-12-28T02:40:21.045734Z","shell.execute_reply":"2025-12-28T02:40:28.797158Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# nnx.display(lora_policy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T02:55:28.507122Z","iopub.execute_input":"2025-12-26T02:55:28.507479Z","iopub.status.idle":"2025-12-26T02:56:08.726Z","shell.execute_reply.started":"2025-12-26T02:55:28.50746Z","shell.execute_reply":"2025-12-26T02:56:08.724815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define reward functions\n\nWe define four reward functions:\n\n- reward if the format of the output exactly matches the instruction given in\n`TEMPLATE`;\n- reward if the format of the output approximately matches the instruction given\nin `TEMPLATE`;\n- reward if the answer is correct/partially correct;\n- Sometimes, the text between `<answer>`, `</answer>` might not be one\n  number. So, we extract the number, and reward the model if the answer is correct.\n\nThe reward functions are inspired from\n[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n\nFirst off, let's define a RegEx for checking whether the format matches.","metadata":{"id":"zLzR1tJfTn1k"}},{"cell_type":"code","source":"match_format = re.compile(\n    rf\"^[\\s]{{0,}}\"\n    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n    rf\"{solution_start}(.+?){solution_end}\"\n    rf\"[\\s]{{0,}}$\",\n    flags=re.MULTILINE | re.DOTALL,\n)\n\nmatch_format.search(\n    f\"{reasoning_start}Let me\"\n    f\" think!{reasoning_end}{solution_start}2{solution_end}\",\n)","metadata":{"id":"C7Beft8wTn1k","outputId":"a0ba4233-562d-485d-b9ba-2f22cf2785b4","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:40:39.805566Z","iopub.execute_input":"2025-12-28T02:40:39.805834Z","iopub.status.idle":"2025-12-28T02:40:39.810317Z","shell.execute_reply.started":"2025-12-28T02:40:39.805815Z","shell.execute_reply":"2025-12-28T02:40:39.809539Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<re.Match object; span=(0, 54), match='<reasoning>Let me think!</reasoning><answer>2</an>"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"Give the model a reward of 3 points if the format matches exactly.","metadata":{"id":"Fe1rF15zTn1k"}},{"cell_type":"code","source":"def match_format_exactly(prompts, completions, **kwargs):\n  return [\n      0 if match_format.search(response) is None else 3.0\n      for response in completions\n  ]","metadata":{"id":"_fhQ6pY2Tn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:40:50.640408Z","iopub.execute_input":"2025-12-28T02:40:50.640696Z","iopub.status.idle":"2025-12-28T02:40:50.643848Z","shell.execute_reply.started":"2025-12-28T02:40:50.640677Z","shell.execute_reply":"2025-12-28T02:40:50.643182Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"We also reward the model if the format of the output matches partially.","metadata":{"id":"sWdAdUHuTn1k"}},{"cell_type":"code","source":"def match_format_approximately(prompts, completions, **kwargs):\n  scores = []\n\n  for completion in completions:\n    score = 0\n    response = completion\n    # Count how many keywords are seen - we penalize if too many!\n    # If we see 1, then plus some points!\n    score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n    score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n    score += 0.5 if response.count(solution_start) == 1 else -0.5\n    score += 0.5 if response.count(solution_end) == 1 else -0.5\n    scores.append(score)\n  return scores","metadata":{"id":"uOhO4f3-Tn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:41:00.800382Z","iopub.execute_input":"2025-12-28T02:41:00.80069Z","iopub.status.idle":"2025-12-28T02:41:00.804652Z","shell.execute_reply.started":"2025-12-28T02:41:00.80067Z","shell.execute_reply":"2025-12-28T02:41:00.803713Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"Reward the model if the answer is correct. A reward is also given if the answer\ndoes not match exactly, i.e., based on how close the answer is to the correct\nvalue.","metadata":{"id":"A2fNZDgTTn1k"}},{"cell_type":"code","source":"def check_answer(prompts, completions, answer, **kwargs):\n  responses = completions\n\n  extracted_responses = [\n      guess.group(1) if (guess := match_format.search(r)) is not None else None\n      for r in responses\n  ]\n\n  scores = []\n  assert len(extracted_responses) == len(\n      answer\n  ), f\"{extracted_responses} and {answer} have mismatching length\"\n  for guess, true_answer in zip(extracted_responses, answer):\n    score = 0\n    if guess is None:\n      scores.append(0)\n      continue\n    # Correct answer gets 3 points!\n    if guess == true_answer:\n      score += 3.0\n    # Match if spaces are seen\n    elif guess.strip() == true_answer.strip():\n      score += 1.5\n    else:\n      # We also reward it if the answer is close via ratios!\n      # Ie if the answer is within some range, reward it!\n      try:\n        ratio = float(guess) / float(true_answer)\n        if ratio >= 0.9 and ratio <= 1.1:\n          score += 0.5\n        elif ratio >= 0.8 and ratio <= 1.2:\n          score += 0.25\n        else:\n          score -= 1.0  # Penalize wrong answers\n      except:\n        score -= 0.5  # Penalize\n    scores.append(score)\n  return scores","metadata":{"id":"S8zcWsmhTn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:41:11.685405Z","iopub.execute_input":"2025-12-28T02:41:11.685701Z","iopub.status.idle":"2025-12-28T02:41:11.691474Z","shell.execute_reply.started":"2025-12-28T02:41:11.685681Z","shell.execute_reply":"2025-12-28T02:41:11.689761Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"Sometimes, the text between `<answer>` and `</answer>` might not be one\nnumber; it can be a sentence. So, we extract the number and compare the answer.","metadata":{"id":"nIpOVv78Tn1k"}},{"cell_type":"code","source":"match_numbers = re.compile(\n    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n)\nmatch_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")","metadata":{"id":"NXvRtbk8Tn1k","outputId":"1ab45f0e-d04a-455c-a046-7c7a1ec6ee22","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:42:16.833324Z","iopub.execute_input":"2025-12-28T02:42:16.833657Z","iopub.status.idle":"2025-12-28T02:42:16.837647Z","shell.execute_reply.started":"2025-12-28T02:42:16.833624Z","shell.execute_reply":"2025-12-28T02:42:16.836971Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"['0.34']"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"def check_numbers(prompts, completions, answer, **kwargs):\n  question = kwargs[\"question\"]\n  responses = completions\n\n  extracted_responses = [\n      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n      for r in responses\n  ]\n\n  scores = []\n  print(\"START ============================\")\n  print(f\"Question: {question[0]}\")\n  print(f\"Answer: {answer[0]}\")\n  print(f\"Response: {responses[0]}\")\n  print(f\"Extracted: {extracted_responses[0]}\")\n  print(\"END ==============================\")\n  for guess, true_answer in zip(extracted_responses, answer):\n    if guess is None:\n      scores.append(0)\n      continue\n    # Convert to numbers\n    try:\n      true_answer = float(true_answer.strip())\n      guess = float(guess.strip())\n      scores.append(1.5 if guess == true_answer else 0.0)\n    except:\n      scores.append(0)\n      continue\n  return scores","metadata":{"id":"oxZQAFKOTn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:41:33.580628Z","iopub.execute_input":"2025-12-28T02:41:33.580892Z","iopub.status.idle":"2025-12-28T02:41:33.585571Z","shell.execute_reply.started":"2025-12-28T02:41:33.580872Z","shell.execute_reply":"2025-12-28T02:41:33.584801Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## Evaluate\n\n\nBefore we train the model, let's evaluate the model on the test set so we can\nsee the improvement post training.\n\nWe evaluate it in two ways:\n\n**Quantitative**\n\n* **Answer Accuracy**: percentage of samples for which the model predicts the\ncorrect final numerical answer  \n* **Answer (Partial) Accuracy**: percentage of samples for which the model\npredicts a final numerical answer such that the \\`model answer / answer\\`\nratio lies between 0.9 and 1.1.  \n* **Format Accuracy**: percentage of samples for which the model outputs the\ncorrect format, i.e., reasoning between the reasoning special tokens, and the\nfinal answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n\n**Qualitative**\n\nWe'll also print outputs for a few given questions so that we can compare the generated output later.\n","metadata":{"id":"AaiYMJxFTn1k"}},{"cell_type":"markdown","source":"We define a helper function to generate an answer, given a prompt.","metadata":{"id":"HAaZ7NjBx99P"}},{"cell_type":"code","source":"def generate(\n    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n):\n  \"\"\"Given prompt, generates text.\"\"\"\n\n  if isinstance(question, str):\n    input_batch = [\n        TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=question,\n        ),\n    ]\n  else:\n    input_batch = [\n        TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=q,\n        )\n        for q in question\n    ]\n\n  out_data = sampler(\n      input_strings=input_batch,\n      max_generation_steps=768,\n      temperature=temperature,\n      top_k=top_k,\n      top_p=top_p,\n      echo=False,\n      seed=seed if seed is not None else None,\n      eos_tokens=[1,106],\n  )\n\n  output = out_data.text\n  if isinstance(question, str):\n    return output[0]\n  return output","metadata":{"id":"_k58bOicUHJy","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:57:45.136499Z","iopub.execute_input":"2025-12-28T02:57:45.136777Z","iopub.status.idle":"2025-12-28T02:57:45.141142Z","shell.execute_reply.started":"2025-12-28T02:57:45.136759Z","shell.execute_reply":"2025-12-28T02:57:45.140286Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"print(generate(\n    \"If a car travels 120 km in 2 hours, what is its speed?\",\n    sampler,\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:58:09.774877Z","iopub.execute_input":"2025-12-28T02:58:09.775146Z","iopub.status.idle":"2025-12-28T02:58:09.845001Z","shell.execute_reply.started":"2025-12-28T02:58:09.775128Z","shell.execute_reply":"2025-12-28T02:58:09.844179Z"}},"outputs":[{"name":"stdout","text":"Please provide the problem! I need the problem to be able to solve it.\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"Another helper function for evaluation.","metadata":{"id":"zNoa5je7yJOt"}},{"cell_type":"code","source":"def evaluate(\n    dataset,\n    sampler,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.95,\n    num_passes=1,\n    corr_lst=False,\n    make_lst=False,\n):\n  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n\n  response_lst = []\n  corr = 0\n  partially_corr = 0\n  corr_format = 0\n  total = 0\n\n  for batch in tqdm(dataset):\n    answers = batch[\"answer\"]\n    questions = batch[\"question\"]\n\n    multiple_call_responses = [[] for _ in range(len(questions))]\n    for p in range(num_passes):\n      responses = generate(\n          questions, sampler, temperature, top_k, top_p, seed=p\n      )\n      for idx, response in enumerate(responses):\n        multiple_call_responses[idx].append(response)\n\n    for question, multiple_call_response, answer in zip(\n        questions, multiple_call_responses, answers\n    ):\n      # check answer\n      corr_ctr_per_question = 0\n      partially_corr_per_question = 0\n      corr_format_per_question = 0\n      for response in multiple_call_response:\n        extracted_response = (\n            guess.group(1)\n            if (guess := match_numbers.search(response)) is not None\n            else \"-1000000\"\n        )\n        try:\n          if float(extracted_response.strip()) == float(answer.strip()):\n            corr_ctr_per_question += 1\n\n          ratio = float(extracted_response.strip()) / float(answer.strip())\n          if ratio >= 0.9 and ratio <= 1.1:\n            partially_corr_per_question += 1\n        except:\n          print(\"SKIPPED\")\n\n        # check format\n        if match_format.search(response) is not None:\n          corr_format_per_question += 1\n\n        if (\n            corr_ctr_per_question > 0\n            and partially_corr_per_question > 0\n            and corr_format_per_question > 0\n        ):\n          break\n\n      if corr_ctr_per_question > 0:\n        corr += 1\n        if corr_lst and make_lst:\n          response_lst.append((question, answer, multiple_call_response))\n      else:\n        if not corr_lst and make_lst:\n          response_lst.append((question, answer, multiple_call_response))\n      if partially_corr_per_question > 0:\n        partially_corr += 1\n      if corr_format_per_question > 0:\n        corr_format += 1\n\n      total += 1\n      if total % 10 == 0:\n        print(\n            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n        )\n\n  to_return = (\n      corr,\n      total,\n      corr / total * 100,\n      partially_corr / total * 100,\n      corr_format / total * 100,\n  )\n  if make_lst:\n    return to_return, response_lst\n  return to_return","metadata":{"id":"yJo2nuKB-wlw","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:42:52.215215Z","iopub.execute_input":"2025-12-28T02:42:52.215485Z","iopub.status.idle":"2025-12-28T02:42:52.222497Z","shell.execute_reply.started":"2025-12-28T02:42:52.215466Z","shell.execute_reply":"2025-12-28T02:42:52.221796Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)","metadata":{"id":"HZMO-KflTn1k","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:43:29.440753Z","iopub.execute_input":"2025-12-28T02:43:29.441042Z","iopub.status.idle":"2025-12-28T02:43:29.492656Z","shell.execute_reply.started":"2025-12-28T02:43:29.441025Z","shell.execute_reply":"2025-12-28T02:43:29.491693Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"Now let's see how the original model does on the test set. You can see the percentages of the mode outputs that are fully correct, partially correct and just correct in format. The following step might take couple of minutes to finish.","metadata":{"id":"UOAQe06DyVlQ"}},{"cell_type":"code","source":"# The evaluation might take up to couple of minutes to finish. Please be patient.\n\n(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n    test_dataset,\n    sampler,\n    **GENERATION_CONFIGS[\"greedy\"],\n)\nprint(\n    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n    f\" {format_accuracy=}%\"\n)","metadata":{"id":"YQM-tzXWUmoE","outputId":"d125e6b1-f940-44b3-a05d-f837d2299f49","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:43:35.72507Z","iopub.execute_input":"2025-12-28T02:43:35.725313Z","iopub.status.idle":"2025-12-28T02:46:11.255326Z","shell.execute_reply.started":"2025-12-28T02:43:35.725295Z","shell.execute_reply":"2025-12-28T02:46:11.254017Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec6ec56aa609412bb2ffab99c41a92f3"}},"metadata":{}},{"name":"stdout","text":"===> corr=1, total=10, corr / total * 100=10.0, partially_corr / total * 100=10.0, corr_format / total * 100=30.0\n===> corr=3, total=20, corr / total * 100=15.0, partially_corr / total * 100=15.0, corr_format / total * 100=35.0\n===> corr=6, total=30, corr / total * 100=20.0, partially_corr / total * 100=20.0, corr_format / total * 100=33.33333333333333\n===> corr=7, total=40, corr / total * 100=17.5, partially_corr / total * 100=20.0, corr_format / total * 100=40.0\n===> corr=7, total=50, corr / total * 100=14.000000000000002, partially_corr / total * 100=18.0, corr_format / total * 100=42.0\n===> corr=9, total=60, corr / total * 100=15.0, partially_corr / total * 100=18.333333333333332, corr_format / total * 100=40.0\n===> corr=12, total=70, corr / total * 100=17.142857142857142, partially_corr / total * 100=20.0, corr_format / total * 100=40.0\n===> corr=14, total=80, corr / total * 100=17.5, partially_corr / total * 100=20.0, corr_format / total * 100=41.25\nSKIPPED\n===> corr=15, total=90, corr / total * 100=16.666666666666664, partially_corr / total * 100=18.88888888888889, corr_format / total * 100=41.11111111111111\n===> corr=17, total=100, corr / total * 100=17.0, partially_corr / total * 100=19.0, corr_format / total * 100=40.0\nSKIPPED\n===> corr=19, total=110, corr / total * 100=17.272727272727273, partially_corr / total * 100=19.090909090909093, corr_format / total * 100=42.72727272727273\n===> corr=21, total=120, corr / total * 100=17.5, partially_corr / total * 100=19.166666666666668, corr_format / total * 100=44.166666666666664\n===> corr=22, total=130, corr / total * 100=16.923076923076923, partially_corr / total * 100=19.230769230769234, corr_format / total * 100=45.38461538461539\nSKIPPED\nSKIPPED\n===> corr=23, total=140, corr / total * 100=16.428571428571427, partially_corr / total * 100=18.571428571428573, corr_format / total * 100=45.714285714285715\nSKIPPED\n===> corr=24, total=150, corr / total * 100=16.0, partially_corr / total * 100=18.0, corr_format / total * 100=45.33333333333333\n===> corr=26, total=160, corr / total * 100=16.25, partially_corr / total * 100=18.125, corr_format / total * 100=45.625\n===> corr=29, total=170, corr / total * 100=17.058823529411764, partially_corr / total * 100=18.823529411764707, corr_format / total * 100=45.88235294117647\n===> corr=30, total=180, corr / total * 100=16.666666666666664, partially_corr / total * 100=18.88888888888889, corr_format / total * 100=46.111111111111114\n===> corr=31, total=190, corr / total * 100=16.315789473684212, partially_corr / total * 100=18.421052631578945, corr_format / total * 100=46.31578947368421\n===> corr=33, total=200, corr / total * 100=16.5, partially_corr / total * 100=18.5, corr_format / total * 100=47.0\n===> corr=35, total=210, corr / total * 100=16.666666666666664, partially_corr / total * 100=18.571428571428573, corr_format / total * 100=46.19047619047619\n===> corr=36, total=220, corr / total * 100=16.363636363636363, partially_corr / total * 100=18.636363636363637, corr_format / total * 100=45.909090909090914\n===> corr=36, total=230, corr / total * 100=15.65217391304348, partially_corr / total * 100=17.82608695652174, corr_format / total * 100=44.78260869565218\n===> corr=37, total=240, corr / total * 100=15.416666666666668, partially_corr / total * 100=17.916666666666668, corr_format / total * 100=45.0\n===> corr=39, total=250, corr / total * 100=15.6, partially_corr / total * 100=18.0, corr_format / total * 100=45.6\n===> corr=39, total=260, corr / total * 100=15.0, partially_corr / total * 100=17.307692307692307, corr_format / total * 100=46.15384615384615\n===> corr=40, total=270, corr / total * 100=14.814814814814813, partially_corr / total * 100=17.037037037037038, corr_format / total * 100=46.666666666666664\n===> corr=40, total=280, corr / total * 100=14.285714285714285, partially_corr / total * 100=16.428571428571427, corr_format / total * 100=46.07142857142857\n===> corr=41, total=290, corr / total * 100=14.13793103448276, partially_corr / total * 100=16.206896551724135, corr_format / total * 100=45.51724137931035\n===> corr=41, total=300, corr / total * 100=13.666666666666666, partially_corr / total * 100=15.666666666666668, corr_format / total * 100=45.33333333333333\n===> corr=43, total=310, corr / total * 100=13.870967741935484, partially_corr / total * 100=15.806451612903224, corr_format / total * 100=46.774193548387096\nSKIPPED\n===> corr=43, total=320, corr / total * 100=13.4375, partially_corr / total * 100=15.312500000000002, corr_format / total * 100=47.1875\nSKIPPED\n===> corr=47, total=330, corr / total * 100=14.242424242424242, partially_corr / total * 100=16.060606060606062, corr_format / total * 100=46.36363636363636\n===> corr=48, total=340, corr / total * 100=14.117647058823529, partially_corr / total * 100=15.88235294117647, corr_format / total * 100=46.470588235294116\n===> corr=50, total=350, corr / total * 100=14.285714285714285, partially_corr / total * 100=16.0, corr_format / total * 100=46.285714285714285\n===> corr=51, total=360, corr / total * 100=14.166666666666666, partially_corr / total * 100=15.833333333333332, corr_format / total * 100=45.83333333333333\n===> corr=53, total=370, corr / total * 100=14.324324324324325, partially_corr / total * 100=15.945945945945947, corr_format / total * 100=46.48648648648649\n===> corr=55, total=380, corr / total * 100=14.473684210526317, partially_corr / total * 100=16.05263157894737, corr_format / total * 100=46.31578947368421\n===> corr=56, total=390, corr / total * 100=14.358974358974358, partially_corr / total * 100=15.897435897435896, corr_format / total * 100=46.41025641025641\n===> corr=58, total=400, corr / total * 100=14.499999999999998, partially_corr / total * 100=16.0, corr_format / total * 100=46.25\ncorr=58, total=400, accuracy=14.499999999999998%, partial_accuracy=16.0%, format_accuracy=46.25%\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"print(generate(\n    \"If a car travels 120 km in 2 hours, what is its speed?\",\n    sampler,\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:51:11.875076Z","iopub.execute_input":"2025-12-28T02:51:11.875359Z","iopub.status.idle":"2025-12-28T02:51:11.9456Z","shell.execute_reply.started":"2025-12-28T02:51:11.875338Z","shell.execute_reply":"2025-12-28T02:51:11.944723Z"}},"outputs":[{"name":"stdout","text":"Please provide the problem! I need the problem to be able to solve it.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"## Train\n\nLet's set up all the configs first - checkpointing, metric logging and training.\nWe then train the model.","metadata":{"id":"-CmB2ZT9Tn1l"}},{"cell_type":"code","source":"# Ckpt saving\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n)\n\n# Metrics logger\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_dir=\"/tmp/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n)","metadata":{"id":"mHzdsYsGTn1l","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:46:49.970833Z","iopub.execute_input":"2025-12-28T02:46:49.971136Z","iopub.status.idle":"2025-12-28T02:46:49.974816Z","shell.execute_reply.started":"2025-12-28T02:46:49.971117Z","shell.execute_reply":"2025-12-28T02:46:49.973919Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Optimizer, learning rate scheduler, gradient clipping\noptimizer = optax.adamw(\n    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=LEARNING_RATE,\n        warmup_steps=WARMUP_STEPS,\n        decay_steps=MAX_STEPS,\n        end_value=0.0,\n    ),\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\nif MAX_GRAD_NORM is not None:\n  optimizer = optax.chain(\n      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n      optimizer,\n  )","metadata":{"id":"YWvBkWBsruom","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:47:02.23042Z","iopub.execute_input":"2025-12-28T02:47:02.230811Z","iopub.status.idle":"2025-12-28T02:47:02.235002Z","shell.execute_reply.started":"2025-12-28T02:47:02.230789Z","shell.execute_reply":"2025-12-28T02:47:02.234009Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Training config\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        # metrics logging\n        metrics_logging_options=metrics_logging_options,\n        # checkpoint saving\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=checkpointing_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n        eos_tokens=[1,106],\n    ),\n)\n\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n)","metadata":{"id":"_6VxFW1ZTn1l","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:47:17.256368Z","iopub.execute_input":"2025-12-28T02:47:17.256715Z","iopub.status.idle":"2025-12-28T02:47:17.261396Z","shell.execute_reply.started":"2025-12-28T02:47:17.256694Z","shell.execute_reply":"2025-12-28T02:47:17.260418Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"### Setting Up the GRPO Trainer\n\nNow we initialize our system for training. First, we create an `RLCluster` instance, which brings together the **policy model (`actor`)**, a **reference model (`reference`)**, and a **tokenizer**. Our `actor` is a trainable LoRA model, while the `reference` is a fixed base model that we use to guide the training.\n\nWe then create a `GRPOLearner`, the specialized trainer that uses a list of **reward functions** to evaluate and optimize the model's output, completing the RL training setup.\n\nTunix trainers are integrated with [Weights & Biases](https://wandb.ai/) to help you visualize the training progress. You can choose how you want to use it:\n\n**Option 1 (Type 1)**: If you're running a quick experiment or just testing things out, choose this. It creates a temporary, private dashboard right in your browser without requiring you to log in or create an account.\n\n**Option 2 (Type 2)**: If you have an existing W&B account and want to save your project's history to your personal dashboard, choose this. You'll be prompted to enter your API key or log in.","metadata":{"id":"z4yJWiElSmOy"}},{"cell_type":"code","source":"# RL cluster\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\n# GRPO Trainer\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[\n        match_format_exactly,\n        match_format_approximately,\n        check_answer,\n        check_numbers,\n    ],\n    grpo_config=grpo_config,\n)","metadata":{"id":"OIe1lO08Tn1l","outputId":"017a2a3f-d9fd-4ac8-87f5-760e2586dfc9","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:59:42.401306Z","iopub.execute_input":"2025-12-28T02:59:42.401899Z","iopub.status.idle":"2025-12-28T02:59:46.73289Z","shell.execute_reply.started":"2025-12-28T02:59:42.40188Z","shell.execute_reply":"2025-12-28T02:59:46.73186Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>jax/core/compile/backend_compile_duration</td><td>‚ñÅ</td></tr><tr><td>jax/core/compile/jaxpr_to_mlir_module_duration</td><td>‚ñÅ</td></tr><tr><td>jax/core/compile/jaxpr_trace_duration</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>jax/core/compile/backend_compile_duration</td><td>1766890161.94149</td></tr><tr><td>jax/core/compile/jaxpr_to_mlir_module_duration</td><td>1766890161.93501</td></tr><tr><td>jax/core/compile/jaxpr_trace_duration</td><td>1766890161.93323</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">2025-12-28_02-48-03</strong> at: <a href='https://wandb.ai/dharshith657-harshiis/tunix/runs/iezhsdbd?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">https://wandb.ai/dharshith657-harshiis/tunix/runs/iezhsdbd?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6</a><br> View project at: <a href='https://wandb.ai/dharshith657-harshiis/tunix?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">https://wandb.ai/dharshith657-harshiis/tunix?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251228_024803-iezhsdbd/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251228_025943-2f5zols8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dharshith657-harshiis/tunix/runs/2f5zols8?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">2025-12-28_02-59-43</a></strong> to <a href='https://wandb.ai/dharshith657-harshiis/tunix?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dharshith657-harshiis/tunix?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">https://wandb.ai/dharshith657-harshiis/tunix?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dharshith657-harshiis/tunix/runs/2f5zols8?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">https://wandb.ai/dharshith657-harshiis/tunix/runs/2f5zols8?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">2025-12-28_02-59-43</strong> at: <a href='https://wandb.ai/dharshith657-harshiis/tunix/runs/2f5zols8?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">https://wandb.ai/dharshith657-harshiis/tunix/runs/2f5zols8?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6</a><br> View project at: <a href='https://wandb.ai/dharshith657-harshiis/tunix?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">https://wandb.ai/dharshith657-harshiis/tunix?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251228_025943-2f5zols8/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251228_025944-z2x57h3b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dharshith657-harshiis/tunix/runs/z2x57h3b?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">2025-12-28_02-59-44</a></strong> to <a href='https://wandb.ai/dharshith657-harshiis/tunix?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dharshith657-harshiis/tunix?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">https://wandb.ai/dharshith657-harshiis/tunix?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dharshith657-harshiis/tunix/runs/z2x57h3b?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6' target=\"_blank\">https://wandb.ai/dharshith657-harshiis/tunix/runs/z2x57h3b?apiKey=419794908deb22729f411a4693c3bd8c740d1fd6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}}],"execution_count":52},{"cell_type":"markdown","source":"The first couple of training step might take up to 5 minutes to finish. Please be patient. If you experience long training steps, e.g. >10 minutes per step, please open a bug. Really appreciated!","metadata":{"id":"e8b71ed5"}},{"cell_type":"code","source":"!ls /tmp/content/ckpts/\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:48:18.719959Z","iopub.execute_input":"2025-12-28T02:48:18.720259Z","iopub.status.idle":"2025-12-28T02:48:19.348657Z","shell.execute_reply.started":"2025-12-28T02:48:18.72024Z","shell.execute_reply":"2025-12-28T02:48:19.347757Z"}},"outputs":[{"name":"stdout","text":"actor\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"print(generate(\n    \"If a car travels 120 km in 2 hours, what is its speed?\",\n    sampler,\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T02:48:57.16533Z","iopub.execute_input":"2025-12-28T02:48:57.166096Z","iopub.status.idle":"2025-12-28T02:49:22.004834Z","shell.execute_reply.started":"2025-12-28T02:48:57.166076Z","shell.execute_reply":"2025-12-28T02:49:22.003929Z"}},"outputs":[{"name":"stdout","text":"Please provide the problem! I need the problem to be able to solve it.\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"with mesh:\n  grpo_trainer.train(train_dataset)","metadata":{"id":"S27XDebYTn1l","outputId":"2869f47f-c7d7-4bcc-b167-f05c77a619fa","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T03:03:56.266254Z","iopub.execute_input":"2025-12-28T03:03:56.266585Z","iopub.status.idle":"2025-12-28T03:07:30.393339Z","shell.execute_reply.started":"2025-12-28T03:03:56.266566Z","shell.execute_reply":"2025-12-28T03:07:30.392013Z"}},"outputs":[{"name":"stdout","text":"START ============================\nQuestion: Maria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?\nAnswer: 3\nResponse: Okay, let's break this problem down.\n\n<reasoning>\nMaria initially had 4 dimes + 4 quarters + 7 nickels = 15 coins.  Her mom gives her 5 quarters, increasing her total to 15 + 5 = 20 coins.  The value of the dimes is 4 * $0.10 = $0.40. The value of the quarters is 4 * $0.25 = $1.00. The value of the nickels is 7 * $0.05 = $0.35.  The total value of her coins is $0.40 + $1.00 + $0.35 = $1.75.  After her mom gives her quarters, she now has 20 + 5 = 25 coins. The value of the quarters is now 5 * $0.25 = $1.25. The total value of her coins is $1.75 + $1.25 = $3.00. Therefore, Maria has $3.00. </answer>\n</reasoning>\n<answer>3.00</answer>\nExtracted: 3.00\nEND ==============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Actor Training:   0%|          | 0/3738 [00:00<?, ?step/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fbb9b5ef02e4b489d05a8ef7fad2ad5"}},"metadata":{}},{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mesh:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m   \u001b[43mgrpo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/grpo/grpo_learner.py:366\u001b[39m, in \u001b[36mGRPOLearner.train\u001b[39m\u001b[34m(self, train_ds, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    321\u001b[39m     train_ds: Iterable[TrainingInputT],\n\u001b[32m    322\u001b[39m     eval_ds: Iterable[TrainingInputT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    323\u001b[39m     skip_jit: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    324\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    325\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"GRPO training loop.\u001b[39;00m\n\u001b[32m    326\u001b[39m \n\u001b[32m    327\u001b[39m \u001b[33;03m  Algorithm as below: extract from https://arxiv.org/abs/2402.03300 ::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    364\u001b[39m \u001b[33;03m    skip_jit: Whether to skip JIT compilation of the training loop.\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_jit\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_learner.py:657\u001b[39m, in \u001b[36mRLLearner.train\u001b[39m\u001b[34m(self, train_ds, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    646\u001b[39m   \u001b[38;5;28mself\u001b[39m._prepare_data(\n\u001b[32m    647\u001b[39m       iterator=\u001b[38;5;28miter\u001b[39m(eval_ds),\n\u001b[32m    648\u001b[39m       proceed_num_steps=-\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    654\u001b[39m       mode=rl_cluster_lib.Mode.EVAL,\n\u001b[32m    655\u001b[39m   )\n\u001b[32m    656\u001b[39m   curr_eval_ds = eval_data_queue.get(block=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrl_cluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurr_train_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurr_eval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_jit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# loop over Œº\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.rl_cluster, \u001b[33m\"\u001b[39m\u001b[33mcritic_trainer\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    663\u001b[39m   \u001b[38;5;28mself\u001b[39m.rl_cluster.update_critic(\n\u001b[32m    664\u001b[39m       curr_train_ds,\n\u001b[32m    665\u001b[39m       curr_eval_ds,\n\u001b[32m    666\u001b[39m       skip_jit,\n\u001b[32m    667\u001b[39m   )  \u001b[38;5;66;03m# loop over Œº\u001b[39;00m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_cluster.py:630\u001b[39m, in \u001b[36mRLCluster.update_actor\u001b[39m\u001b[34m(self, train_ds, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cluster_config.role_to_mesh[Role.ACTOR]:\n\u001b[32m    629\u001b[39m   \u001b[38;5;28mself\u001b[39m._maybe_load_model_from_cpu(\u001b[38;5;28mself\u001b[39m.actor_trainer.model, Role.ACTOR)\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_jit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m   \u001b[38;5;28mself\u001b[39m._maybe_offload_model_to_cpu(\u001b[38;5;28mself\u001b[39m.actor_trainer.model, Role.ACTOR)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/sft/peft_trainer.py:625\u001b[39m, in \u001b[36mPeftTrainer.train\u001b[39m\u001b[34m(self, train_ds, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._flops_measured \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_jit:\n\u001b[32m    623\u001b[39m   \u001b[38;5;28mself\u001b[39m._flops_measured = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m   tflops_per_step = \u001b[43msystem_metrics_calculator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasure_tflops_per_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtrain_step_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtrain_example\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m tflops_per_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    632\u001b[39m     \u001b[38;5;28mself\u001b[39m.metrics_logger.log(\n\u001b[32m    633\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtflops_per_step\u001b[39m\u001b[33m\"\u001b[39m, tflops_per_step, \u001b[38;5;28mself\u001b[39m._mode, \u001b[32m0\u001b[39m\n\u001b[32m    634\u001b[39m     )\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/sft/system_metrics_calculator.py:38\u001b[39m, in \u001b[36mmeasure_tflops_per_step\u001b[39m\u001b[34m(train_step_fn, model, optimizer, train_example)\u001b[39m\n\u001b[32m     35\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m   compiled = \u001b[43mtrain_step_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_example\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m   cost = compiled.cost_analysis()\n\u001b[32m     40\u001b[39m   flops = cost.get(\u001b[33m\"\u001b[39m\u001b[33mflops\u001b[39m\u001b[33m\"\u001b[39m)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/flax/nnx/transforms/compilation.py:649\u001b[39m, in \u001b[36mLowered.compile\u001b[39m\u001b[34m(self, compiler_options)\u001b[39m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile\u001b[39m(\n\u001b[32m    646\u001b[39m   \u001b[38;5;28mself\u001b[39m, compiler_options: jax.stages.CompilerOptions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    647\u001b[39m ) -> Compiled:\n\u001b[32m    648\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Compile, returning a corresponding ``Compiled`` instance.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m   compiled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlowered\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    650\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m Compiled(compiled, \u001b[38;5;28mself\u001b[39m.jit_wrapped)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/jax/_src/stages.py:628\u001b[39m, in \u001b[36mLowered.compile\u001b[39m\u001b[34m(self, compiler_options, device_assignment)\u001b[39m\n\u001b[32m    621\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compile, returning a corresponding ``Compiled`` instance.\"\"\"\u001b[39;00m\n\u001b[32m    623\u001b[39m kw: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\n\u001b[32m    624\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompiler_options\u001b[39m\u001b[33m\"\u001b[39m: compiler_options,\n\u001b[32m    625\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdevice_assignment\u001b[39m\u001b[33m\"\u001b[39m: device_assignment\n\u001b[32m    626\u001b[39m }\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Compiled(\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lowering\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# pytype: disable=wrong-keyword-args\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28mself\u001b[39m._lowering.const_args,\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m.args_info,\n\u001b[32m    631\u001b[39m     \u001b[38;5;28mself\u001b[39m.out_tree,\n\u001b[32m    632\u001b[39m     no_kwargs=\u001b[38;5;28mself\u001b[39m._no_kwargs,\n\u001b[32m    633\u001b[39m )\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2513\u001b[39m, in \u001b[36mMeshComputation.compile\u001b[39m\u001b[34m(self, compiler_options, device_assignment)\u001b[39m\n\u001b[32m   2510\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compilation_device_list, (\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), xc.DeviceList))\n\u001b[32m   2512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs \u001b[38;5;129;01mor\u001b[39;00m device_assignment:\n\u001b[32m-> \u001b[39m\u001b[32m2513\u001b[39m   executable = \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2514\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2516\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdevice_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompilation_device_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2517\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[32m   2518\u001b[39m     \u001b[38;5;28mself\u001b[39m._executable = executable\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:3059\u001b[39m, in \u001b[36mUnloadedMeshExecutable.from_hlo\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3056\u001b[39m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3058\u001b[39m util.test_event(\u001b[33m\"\u001b[39m\u001b[33mpxla_cached_compilation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3059\u001b[39m xla_executable = \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[32m   3066\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2840\u001b[39m, in \u001b[36m_cached_compilation\u001b[39m\u001b[34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[39m\n\u001b[32m   2832\u001b[39m compile_options = create_compile_options(\n\u001b[32m   2833\u001b[39m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[32m   2834\u001b[39m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[32m   2835\u001b[39m     dev, pmap_nreps, compiler_options)\n\u001b[32m   2837\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dispatch.log_elapsed_time(\n\u001b[32m   2838\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2839\u001b[39m     fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[32m-> \u001b[39m\u001b[32m2840\u001b[39m   xla_executable = \u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2841\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2842\u001b[39m \u001b[43m      \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/jax/_src/compiler.py:494\u001b[39m, in \u001b[36mcompile_or_get_cached\u001b[39m\u001b[34m(backend, computation, devices, compile_options, host_callbacks, executable_devices, pgle_profiler)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    493\u001b[39m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m      \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/jax/_src/compiler.py:762\u001b[39m, in \u001b[36m_compile_and_write_cache\u001b[39m\u001b[34m(backend, computation, executable_devices, compile_options, host_callbacks, module_name, cache_key)\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_and_write_cache\u001b[39m(\n\u001b[32m    753\u001b[39m     backend: xc.Client,\n\u001b[32m    754\u001b[39m     computation: ir.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    759\u001b[39m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    760\u001b[39m ) -> xc.LoadedExecutable:\n\u001b[32m    761\u001b[39m   start_time = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m   executable = \u001b[43mbackend_compile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m   compile_time = time.monotonic() - start_time\n\u001b[32m    766\u001b[39m   _cache_write(\n\u001b[32m    767\u001b[39m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[32m    768\u001b[39m   )\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/jax/_src/profiler.py:364\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    363\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/jax/_src/compiler.py:378\u001b[39m, in \u001b[36mbackend_compile_and_load\u001b[39m\u001b[34m(backend, module, executable_devices, options, host_callbacks)\u001b[39m\n\u001b[32m    369\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m backend.compile_and_load(\n\u001b[32m    370\u001b[39m           built_c,\n\u001b[32m    371\u001b[39m           executable_devices=executable_devices,\n\u001b[32m    372\u001b[39m           compile_options=options,\n\u001b[32m    373\u001b[39m           host_callbacks=host_callbacks,\n\u001b[32m    374\u001b[39m       )\n\u001b[32m    375\u001b[39m     \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m xc.XlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    384\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n","\u001b[31mKeyboardInterrupt\u001b[39m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":57},{"cell_type":"markdown","source":"## Evaluate\n\nLet's evaluate our finetuned model!","metadata":{"id":"FzIP8glkTn1l"}},{"cell_type":"code","source":"# Load checkpoint first.\nimport re\n\n# Find the latest checkpoint by listing directories in CKPT_DIR/actor\nactor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\n\nlatest_step = -1\nif os.path.exists(actor_ckpt_dir):\n  for item in os.listdir(actor_ckpt_dir):\n    if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n      step = int(item)\n      if step > latest_step:\n        latest_step = step\n\nif latest_step == -1:\n  raise FileNotFoundError(f\"No checkpoints found in {actor_ckpt_dir}\")\n\nprint(f\"Latest checkpoint step: {latest_step}\")\n\nwandb.init(project='tunix-eval')  # logging bug workaround\n\ntrained_ckpt_path = os.path.join(\n    CKPT_DIR, \"actor\", str(latest_step), \"model_params\"\n)\n\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\ntrained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n\nnnx.update(\n    lora_policy,\n    jax.tree.map(\n        lambda a, b: b,\n        nnx.state(lora_policy, nnx.LoRAParam),\n        trained_lora_params,\n    ),\n)","metadata":{"id":"V-73HfP1Tn1l","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T03:23:36.416395Z","iopub.execute_input":"2025-12-28T03:23:36.416748Z","iopub.status.idle":"2025-12-28T03:23:36.456649Z","shell.execute_reply.started":"2025-12-28T03:23:36.416729Z","shell.execute_reply":"2025-12-28T03:23:36.455545Z"}},"outputs":[{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m         latest_step = step\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m latest_step == -\u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo checkpoints found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactor_ckpt_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLatest checkpoint step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m wandb.init(project=\u001b[33m'\u001b[39m\u001b[33mtunix-eval\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# logging bug workaround\u001b[39;00m\n","\u001b[31mFileNotFoundError\u001b[39m: No checkpoints found in /tmp/content/ckpts/actor"],"ename":"FileNotFoundError","evalue":"No checkpoints found in /tmp/content/ckpts/actor","output_type":"error"}],"execution_count":67},{"cell_type":"code","source":"sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)","metadata":{"id":"1vY9kl-ITn1l","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T03:00:12.585572Z","iopub.execute_input":"2025-12-28T03:00:12.58583Z","iopub.status.idle":"2025-12-28T03:00:12.642364Z","shell.execute_reply.started":"2025-12-28T03:00:12.585812Z","shell.execute_reply":"2025-12-28T03:00:12.641519Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# The evaluation might take up to couple of minutes to finish. Please be patient.\n(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n    test_dataset,\n    sampler,\n    **GENERATION_CONFIGS[\"greedy\"],\n)\nprint(\n    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n    f\" {format_accuracy=}%\"\n)","metadata":{"id":"nz0q_gGHqYz6","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T03:00:26.995612Z","iopub.execute_input":"2025-12-28T03:00:26.995888Z","iopub.status.idle":"2025-12-28T03:02:41.894892Z","shell.execute_reply.started":"2025-12-28T03:00:26.995869Z","shell.execute_reply":"2025-12-28T03:02:41.89423Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bd300ff8131407bb28b750d1974bae1"}},"metadata":{}},{"name":"stdout","text":"===> corr=1, total=10, corr / total * 100=10.0, partially_corr / total * 100=10.0, corr_format / total * 100=30.0\n===> corr=3, total=20, corr / total * 100=15.0, partially_corr / total * 100=15.0, corr_format / total * 100=35.0\n===> corr=6, total=30, corr / total * 100=20.0, partially_corr / total * 100=20.0, corr_format / total * 100=33.33333333333333\n===> corr=7, total=40, corr / total * 100=17.5, partially_corr / total * 100=20.0, corr_format / total * 100=40.0\n===> corr=7, total=50, corr / total * 100=14.000000000000002, partially_corr / total * 100=18.0, corr_format / total * 100=42.0\n===> corr=9, total=60, corr / total * 100=15.0, partially_corr / total * 100=18.333333333333332, corr_format / total * 100=40.0\n===> corr=12, total=70, corr / total * 100=17.142857142857142, partially_corr / total * 100=20.0, corr_format / total * 100=40.0\n===> corr=14, total=80, corr / total * 100=17.5, partially_corr / total * 100=20.0, corr_format / total * 100=41.25\nSKIPPED\n===> corr=15, total=90, corr / total * 100=16.666666666666664, partially_corr / total * 100=18.88888888888889, corr_format / total * 100=41.11111111111111\n===> corr=17, total=100, corr / total * 100=17.0, partially_corr / total * 100=19.0, corr_format / total * 100=40.0\nSKIPPED\n===> corr=19, total=110, corr / total * 100=17.272727272727273, partially_corr / total * 100=19.090909090909093, corr_format / total * 100=42.72727272727273\n===> corr=21, total=120, corr / total * 100=17.5, partially_corr / total * 100=19.166666666666668, corr_format / total * 100=44.166666666666664\n===> corr=22, total=130, corr / total * 100=16.923076923076923, partially_corr / total * 100=19.230769230769234, corr_format / total * 100=45.38461538461539\nSKIPPED\nSKIPPED\n===> corr=23, total=140, corr / total * 100=16.428571428571427, partially_corr / total * 100=18.571428571428573, corr_format / total * 100=45.714285714285715\nSKIPPED\n===> corr=24, total=150, corr / total * 100=16.0, partially_corr / total * 100=18.0, corr_format / total * 100=45.33333333333333\n===> corr=26, total=160, corr / total * 100=16.25, partially_corr / total * 100=18.125, corr_format / total * 100=45.625\n===> corr=29, total=170, corr / total * 100=17.058823529411764, partially_corr / total * 100=18.823529411764707, corr_format / total * 100=45.88235294117647\n===> corr=30, total=180, corr / total * 100=16.666666666666664, partially_corr / total * 100=18.88888888888889, corr_format / total * 100=46.111111111111114\n===> corr=31, total=190, corr / total * 100=16.315789473684212, partially_corr / total * 100=18.421052631578945, corr_format / total * 100=46.31578947368421\n===> corr=33, total=200, corr / total * 100=16.5, partially_corr / total * 100=18.5, corr_format / total * 100=47.0\n===> corr=35, total=210, corr / total * 100=16.666666666666664, partially_corr / total * 100=18.571428571428573, corr_format / total * 100=46.19047619047619\n===> corr=36, total=220, corr / total * 100=16.363636363636363, partially_corr / total * 100=18.636363636363637, corr_format / total * 100=45.909090909090914\n===> corr=36, total=230, corr / total * 100=15.65217391304348, partially_corr / total * 100=17.82608695652174, corr_format / total * 100=44.78260869565218\n===> corr=37, total=240, corr / total * 100=15.416666666666668, partially_corr / total * 100=17.916666666666668, corr_format / total * 100=45.0\n===> corr=39, total=250, corr / total * 100=15.6, partially_corr / total * 100=18.0, corr_format / total * 100=45.6\n===> corr=39, total=260, corr / total * 100=15.0, partially_corr / total * 100=17.307692307692307, corr_format / total * 100=46.15384615384615\n===> corr=40, total=270, corr / total * 100=14.814814814814813, partially_corr / total * 100=17.037037037037038, corr_format / total * 100=46.666666666666664\n===> corr=40, total=280, corr / total * 100=14.285714285714285, partially_corr / total * 100=16.428571428571427, corr_format / total * 100=46.07142857142857\n===> corr=41, total=290, corr / total * 100=14.13793103448276, partially_corr / total * 100=16.206896551724135, corr_format / total * 100=45.51724137931035\n===> corr=41, total=300, corr / total * 100=13.666666666666666, partially_corr / total * 100=15.666666666666668, corr_format / total * 100=45.33333333333333\n===> corr=43, total=310, corr / total * 100=13.870967741935484, partially_corr / total * 100=15.806451612903224, corr_format / total * 100=46.774193548387096\nSKIPPED\n===> corr=43, total=320, corr / total * 100=13.4375, partially_corr / total * 100=15.312500000000002, corr_format / total * 100=47.1875\nSKIPPED\n===> corr=47, total=330, corr / total * 100=14.242424242424242, partially_corr / total * 100=16.060606060606062, corr_format / total * 100=46.36363636363636\n===> corr=48, total=340, corr / total * 100=14.117647058823529, partially_corr / total * 100=15.88235294117647, corr_format / total * 100=46.470588235294116\n===> corr=50, total=350, corr / total * 100=14.285714285714285, partially_corr / total * 100=16.0, corr_format / total * 100=46.285714285714285\n===> corr=51, total=360, corr / total * 100=14.166666666666666, partially_corr / total * 100=15.833333333333332, corr_format / total * 100=45.83333333333333\n===> corr=53, total=370, corr / total * 100=14.324324324324325, partially_corr / total * 100=15.945945945945947, corr_format / total * 100=46.48648648648649\n===> corr=55, total=380, corr / total * 100=14.473684210526317, partially_corr / total * 100=16.05263157894737, corr_format / total * 100=46.31578947368421\n===> corr=56, total=390, corr / total * 100=14.358974358974358, partially_corr / total * 100=15.897435897435896, corr_format / total * 100=46.41025641025641\n===> corr=58, total=400, corr / total * 100=14.499999999999998, partially_corr / total * 100=16.0, corr_format / total * 100=46.25\ncorr=58, total=400, accuracy=14.499999999999998%, partial_accuracy=16.0%, format_accuracy=46.25%\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"def generate(question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n    if isinstance(question, str):\n        input_strings = [\n            SYSTEM_PROMPT + \"\\n\\n\" + question\n        ]\n    else:\n        input_strings = [\n            SYSTEM_PROMPT + \"\\n\\n\" + q\n            for q in question\n        ]\n\n    out_data = sampler(\n        input_strings=input_strings,\n        max_generation_steps=768,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        echo=False,\n        seed=seed,\n        eos_tokens=[1, 106],  # Gemma EOS tokens\n    )\n\n    output = out_data.text\n    return output[0] if isinstance(question, str) else output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T03:14:35.015408Z","iopub.execute_input":"2025-12-28T03:14:35.015728Z","iopub.status.idle":"2025-12-28T03:14:35.02058Z","shell.execute_reply.started":"2025-12-28T03:14:35.015709Z","shell.execute_reply":"2025-12-28T03:14:35.019304Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"print(generate(\n    \"If a man travels 12 km in 8 hours, what is its speed?\",\n    sampler,\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T03:27:06.786549Z","iopub.execute_input":"2025-12-28T03:27:06.786874Z","iopub.status.idle":"2025-12-28T03:27:07.215734Z","shell.execute_reply.started":"2025-12-28T03:27:06.786856Z","shell.execute_reply":"2025-12-28T03:27:07.214568Z"}},"outputs":[{"name":"stdout","text":"\n\n<reasoning>The problem asks for the speed of a man. The given information about the distance traveled and the time taken is sufficient to determine the speed. We can calculate the speed by dividing the distance traveled by the time taken. The problem states that the man travels 12 km in 8 hours. Therefore, the speed is 12 km / 8 hours = 1.5 km/hour. </reasoning>\n<answer>1.5</answer>\n\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"With sufficient training, you should see that the percentages of correct model outputs have clearly gone up, which means our training worked.","metadata":{"id":"s1NMAxMh0H5D"}}]}